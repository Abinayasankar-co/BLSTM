{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 30])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = [tensor([ 8913,  7761,  4515, 12954,  6169,  7128, 11620,   942,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]),\n",
    " tensor([16666, 13080, 14772, 18070,  1735, 20429, 18070, 12314, 15726, 18070,\n",
    "         11215,  7761,  1464, 12954, 14706,  1595,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]),\n",
    " tensor([ 4497, 11093,   420,  3083,  2777,  7050, 14586, 20132,  6349,  5997,\n",
    "         12855,  1311, 12577,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]),\n",
    " tensor([ 7196, 15591,  7421,  2300, 10054,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0])]\n",
    "\n",
    "input_ids = torch.stack(X_train)\n",
    "input_ids.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 8913,  7761,  4515, 12954,  6169,  7128, 11620,   942,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0],\n",
       "       [16666, 13080, 14772, 18070,  1735, 20429, 18070, 12314, 15726,\n",
       "        18070, 11215,  7761,  1464, 12954, 14706,  1595,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0],\n",
       "       [ 4497, 11093,   420,  3083,  2777,  7050, 14586, 20132,  6349,\n",
       "         5997, 12855,  1311, 12577,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0],\n",
       "       [ 7196, 15591,  7421,  2300, 10054,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0]], dtype=int64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "X_train = np.array([tensor.numpy() for tensor in input_ids])\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, torch.Size([4, 30]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = torch.LongTensor(X_train).to(\"cuda\")\n",
    "X_train.ndim, X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 8913,  7761,  4515, 12954,  6169,  7128, 11620,   942,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [16666, 13080, 14772, 18070,  1735, 20429, 18070, 12314, 15726, 18070,\n",
       "         11215,  7761,  1464, 12954, 14706,  1595,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [ 4497, 11093,   420,  3083,  2777,  7050, 14586, 20132,  6349,  5997,\n",
       "         12855,  1311, 12577,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [ 7196, 15591,  7421,  2300, 10054,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train  = np.squeeze(X_train)\n",
    "X_train.ndim\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.]], device='cuda:0')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train = tensor([[0., 1., 0.],\n",
    "        [0., 1., 0.],\n",
    "        [1., 0., 0.],\n",
    "        [1., 0., 0.]]).to(\"cuda\")\n",
    "Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-1.5623,  0.3307, -1.7375, -0.6516,  0.6286],\n",
       "        [-0.3498,  1.2804, -1.1967,  0.7937, -2.5152],\n",
       "        [ 0.5925,  0.8923,  0.1275,  0.1657, -1.6927],\n",
       "        ...,\n",
       "        [-1.1267, -0.7866, -0.7257, -0.7439,  0.9918],\n",
       "        [ 0.5385, -0.9795, -1.0433, -1.1132,  1.3873],\n",
       "        [-0.2853,  0.1684,  0.3318,  0.4403,  0.6071]], device='cuda:0',\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_1 = nn.Embedding(20430,5).to(\"cuda\")\n",
    "embedding_1(X_train).mean().backward()\n",
    "embedding_1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.4653, -0.1402, -0.5111,  ..., -0.4226, -0.4358,  0.6931],\n",
       "        [-1.1975, -1.1978, -1.3791,  ...,  0.8466, -0.9747, -0.3541],\n",
       "        [ 1.0532, -1.6744,  1.5458,  ..., -1.4225,  0.4158, -1.7623],\n",
       "        ...,\n",
       "        [ 0.9673, -1.0869, -0.2256,  ..., -0.5339,  0.2311,  1.0115],\n",
       "        [-1.4787,  2.1059,  0.4451,  ...,  0.0455, -1.5949,  1.0055],\n",
       "        [-1.4242, -0.0604, -0.2198,  ...,  1.6808,  0.2180, -1.3261]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_2 = nn.Embedding(20430,30).to(\"cuda\")\n",
    "embedding_2(X_train).mean().backward()\n",
    "embedding_2.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BLSTM(nn.Module):\n",
    "   def __init__(self):\n",
    "    super(BLSTM,self).__init__()\n",
    "    self.Embeddings = nn.Embedding(20430,100)\n",
    "    self.LSTM = nn.LSTM(100,128,bidirectional=True)\n",
    "    self.fc = nn.Linear(128*2,3)\n",
    "    self.dropout = nn.Dropout(0.5)\n",
    "    self.tanh = nn.Tanh()\n",
    "\n",
    "   def forward(self,X):\n",
    "    embed = self.Embeddings(X)\n",
    "    output,(hidden,cell) = self.LSTM(embed)\n",
    "    last_hidden = output[:, -1,:]\n",
    "    print(last_hidden)\n",
    "    logits = self.fc(self.dropout(last_hidden))\n",
    "    return self.tanh(logits)\n",
    "    \n",
    "\n",
    "model = BLSTM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2276,  0.0593,  0.0443,  ...,  0.0228,  0.1420, -0.1686],\n",
      "        [-0.3204,  0.0834,  0.0643,  ...,  0.0131,  0.1222, -0.1395],\n",
      "        [-0.3600,  0.0907,  0.0751,  ...,  0.0015,  0.0923, -0.1034],\n",
      "        [-0.3776,  0.0909,  0.0815,  ..., -0.0079,  0.0494, -0.0587]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch : 1/ 0\n",
      "tensor([[-0.2269,  0.0586,  0.0449,  ...,  0.0237,  0.1437, -0.1713],\n",
      "        [-0.3194,  0.0820,  0.0651,  ...,  0.0139,  0.1235, -0.1417],\n",
      "        [-0.3588,  0.0887,  0.0761,  ...,  0.0021,  0.0932, -0.1049],\n",
      "        [-0.3764,  0.0885,  0.0825,  ..., -0.0075,  0.0498, -0.0594]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch : 2/ 1\n",
      "tensor([[-0.2263,  0.0579,  0.0445,  ...,  0.0237,  0.1433, -0.1724],\n",
      "        [-0.3184,  0.0808,  0.0644,  ...,  0.0140,  0.1231, -0.1426],\n",
      "        [-0.3576,  0.0870,  0.0751,  ...,  0.0021,  0.0928, -0.1055],\n",
      "        [-0.3751,  0.0865,  0.0812,  ..., -0.0075,  0.0495, -0.0597]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch : 3/ 2\n",
      "tensor([[-0.2257,  0.0575,  0.0441,  ...,  0.0235,  0.1424, -0.1716],\n",
      "        [-0.3175,  0.0798,  0.0636,  ...,  0.0138,  0.1222, -0.1420],\n",
      "        [-0.3566,  0.0857,  0.0739,  ...,  0.0020,  0.0922, -0.1051],\n",
      "        [-0.3739,  0.0849,  0.0798,  ..., -0.0076,  0.0492, -0.0595]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch : 4/ 3\n",
      "tensor([[-0.2252,  0.0571,  0.0437,  ...,  0.0232,  0.1413, -0.1701],\n",
      "        [-0.3169,  0.0790,  0.0628,  ...,  0.0135,  0.1213, -0.1407],\n",
      "        [-0.3558,  0.0846,  0.0728,  ...,  0.0017,  0.0915, -0.1042],\n",
      "        [-0.3731,  0.0836,  0.0785,  ..., -0.0078,  0.0488, -0.0590]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch : 5/ 4\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "epochs = 5\n",
    "lr = 3e-5\n",
    "save_path = \"./Output/samplemodel.pth\"\n",
    "model.to(\"cuda\")\n",
    "\n",
    "\n",
    "optimizer = Adam(model.parameters(),lr = lr)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    optimizer.zero_grad()\n",
    "    logits = model(X_train)\n",
    "    loss = criterion(logits,Y_train)\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    train_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch : {epoch+1}/ {epoch}\")\n",
    "\n",
    "torch.save(model.state_dict(),save_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ABINAYA SANKAR M\\AppData\\Local\\Temp\\ipykernel_13900\\944173087.py:22: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"D:/FinalYearProj/backend/Output/samplemodel.pth\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1], device='cuda:0')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class ALSAPredictHard(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ALSAPredictHard,self).__init__()\n",
    "        self.Embeddings = nn.Embedding(20430,100)\n",
    "        self.LSTM = nn.LSTM(100,128,num_layers=1,batch_first=True,bidirectional=True)\n",
    "        self.fc = nn.Linear(128*2,3)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self,X):\n",
    "        embed = self.Embeddings(X)\n",
    "        output , (hidden,cell) = self.LSTM(embed)\n",
    "        last_hidden = output[:,-1,:]\n",
    "        logits = self.fc(self.dropout(last_hidden))\n",
    "        return self.tanh(logits)\n",
    "    \n",
    "\n",
    "model = ALSAPredictHard()\n",
    "model.load_state_dict(torch.load(\"D:/FinalYearProj/backend/Output/samplemodel.pth\"))\n",
    "model.to(\"cuda\")\n",
    "model.eval()\n",
    "\n",
    "def prepare_input(data):\n",
    "    return torch.tensor(data).unsqueeze(0).to('cuda')\n",
    "\n",
    "\n",
    "data_X_test = np.array([ 8913,  7761,  4515, 12954,  6169,  7128, 11620,   942,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
    "\n",
    "X_test = prepare_input(data_X_test)\n",
    "print(X_test.ndim)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(X_test)\n",
    "\n",
    "predictions = torch.argmax(logits,dim=1)\n",
    "predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
